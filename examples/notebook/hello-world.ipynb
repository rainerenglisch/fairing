{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/fairing', '/home/jovyan/fairing', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '', '/opt/conda/lib/python3.6/site-packages', '/opt/conda/lib/python3.6/site-packages/IPython/extensions', '/home/jovyan/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/home/jovyan/fairing\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.debug('This will get logged')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job'.format(GCP_PROJECT)\n",
    "fairing.config.set_builder('append',base_image='tensorflow/tensorflow:latest-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "fairing.config.set_deployer('job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "#GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "#DOCKER_REGISTRY = 'mtr.external.otc.telekomcloud.com/rainer_englisch'\n",
    "#DOCKER_REGISTRY = 'docker.io/m1st3rb3an'\n",
    "#DOCKER_REGISTRY = 'registry.hub.docker.com/m1st3rb3an'\n",
    "DOCKER_REGISTRY = 'index.docker.io/m1st3rb3an'\n",
    "\n",
    "BASE_IMAGE ='tensorflow/tensorflow:nightly-py3-jupyter'\n",
    "#fairing.config.set_builder('append',base_image='tensorflow/tensorflow:latest-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "# Rainer: need to use latest base_image because of newer python. The older version in base image is not compatible with one in notebook in regards to serialization\n",
    "#fairing.config.set_builder('append',base_image='tensorflow/tensorflow:devel-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "#fairing.config.set_builder('append',base_image='tensorflow/tensorflow:1.13.1-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "fairing.config.set_builder('append',base_image=BASE_IMAGE, registry=DOCKER_REGISTRY, push=True)\n",
    "fairing.config.set_deployer('job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains and Evaluates the MNIST network using a feed dictionary.\"\"\"\n",
    "import os\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "\n",
    "#import fairing\n",
    "#fairing.config.set_builder('docker', registry='<your-registry-name>',    base_image='tensorflow/tensorflow:1.13.1-py3')\n",
    "#fairing.config.run()\n",
    "\n",
    "INPUT_DATA_DIR = '/tmp/tensorflow/mnist/input_data/'\n",
    "MAX_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.3\n",
    "HIDDEN_1 = 128\n",
    "HIDDEN_2 = 32\n",
    "\n",
    "# HACK: Ideally we would want to have a unique subpath for each instance of the job, but since we can't\n",
    "# we are instead appending HOSTNAME to the logdir\n",
    "LOG_DIR = os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                       'tensorflow/mnist/logs/fully_connected_feed/', os.getenv('HOSTNAME', ''))\n",
    "MODEL_DIR = os.path.join(LOG_DIR, 'model.ckpt')\n",
    "\n",
    "def train():\n",
    "    \n",
    "    #logger.info(\"Importing libs\")\n",
    "    #import tensorflow as tf\n",
    "    #from tensorflow.examples.tutorials.mnist import input_data\n",
    "    #from tensorflow.examples.tutorials.mnist import mnist\n",
    "    logger.info(\"Start training\")\n",
    "    data_sets = input_data.read_data_sets(INPUT_DATA_DIR)\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(BATCH_SIZE, mnist.IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(BATCH_SIZE))\n",
    "\n",
    "    logits = mnist.inference(images_placeholder, HIDDEN_1, HIDDEN_2)\n",
    "    \n",
    "    loss = mnist.loss(logits, labels_placeholder)\n",
    "    train_op = mnist.training(loss, LEARNING_RATE)\n",
    "    summary = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "    sess.run(init)\n",
    "    \n",
    "    data_set = data_sets.train\n",
    "    logger.info(\"Start training\")\n",
    "    for step in xrange(MAX_STEPS):\n",
    "        images_feed, labels_feed = data_set.next_batch(BATCH_SIZE, False)\n",
    "        feed_dict = {\n",
    "            images_placeholder: images_feed,\n",
    "            labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                                 feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"At step {}, loss = {}\".format(step, loss_value))\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "    logger.info(\"Save my model\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, 'my_mnist_model')\n",
    "    logger.info(\"End training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local training\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <fairing.preprocessors.function.FunctionPreProcessor object at 0x7fad2df564a8>\n",
      "Using builder: <fairing.builders.append.append.AppendBuilder object at 0x7fad600310f0>\n",
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_h6hyrhzq\n",
      "/home/jovyan/fairing/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'tensorflow/tensorflow:nightly-py3-jupyter'\n",
      "Image successfully built in 1.3086363889997301s.\n",
      "Pushing image index.docker.io/m1st3rb3an/fairing-job:22FAAB03...\n",
      "Loading Docker credentials for repository 'index.docker.io/m1st3rb3an/fairing-job:22FAAB03'\n",
      "Uploading index.docker.io/m1st3rb3an/fairing-job:22FAAB03\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:e0b421c621114fc389e8dfa0414c80c2f793e7dc2efe2d449830890bb21afd83 exists, skipping\n",
      "Layer sha256:30c30f323f555817df41239125e472e408a81167b64a7039de0054ec28e33570 exists, skipping\n",
      "Layer sha256:32306fc019abe59ddb505e0b648d83c806bc080f48169987cd0d72e3a8c7db17 exists, skipping\n",
      "Layer sha256:4bbf71d7b8bf0ea8c97466e83817b8119fdd79ac8249a160d5e89ded63ad2305 exists, skipping\n",
      "Layer sha256:b6ee0bfb9b1c368168d47e23c9d657f47b9040cdcf1f49a8819c2adfa50a860d exists, skipping\n",
      "Layer sha256:526bb5d0ae6c26ed478aa60ae185d5652e5c8897638368943d992b78a52b1531 exists, skipping\n",
      "Layer sha256:868fe117e0a60fd3c6c0561ee074e68b8f53339cfb2f56ac511f6880a6c9287f exists, skipping\n",
      "Layer sha256:db7d1dfba811e25d72d24f10c0c98b43775d172401ea7ddea0db5753b0886455 exists, skipping\n",
      "Layer sha256:ec20aa1f601bda33bf3452f5d72e98f118ffbbdd978de9e9be51f1cc4333ca65 exists, skipping\n",
      "Layer sha256:6935b877738aa3f50dc8111baf153e1b4a9425588b26e105e8cb4f350b4999ba exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:e15daa901f894a3f4e4ace41b4b5b7c7458a5b9293b0f3c91c5fcf107badbe89 exists, skipping\n",
      "Layer sha256:017c56521e603ecbd6287c799a3fc682c034b3f30c2b4531baef4b41c21f3f34 exists, skipping\n",
      "Layer sha256:dadf115bb905dfb34282c8ba43fbc833f1320048bb5c21fb4977b535c862238e exists, skipping\n",
      "Layer sha256:435543f18186ca45bb39d5657d6d36ab6ea54372a738996d91024e5d890dc663 exists, skipping\n",
      "Layer sha256:d3f438728c991a3a8d06c7f640f7d142521ef3beda9312dabc92ad09803b2833 exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:ef2fadf9b3940ee85ce37bf07a64871056bc09f82538ae7b195f0a946bdb7e5b exists, skipping\n",
      "Layer sha256:563f85a06a6afea926f1110be2367fa5664bd77fcb7480ac936f514e497962ef exists, skipping\n",
      "Layer sha256:4922b6b5f9430be9267ee7dcdc4277a95c6e108d815bf62e35d0e4dc3e5eccdf pushed.\n",
      "Layer sha256:f24532b699eb3ddee8e59c257affa65e148571be7f4ac3587190d84c1cdfbe08 pushed.\n",
      "Finished upload of: index.docker.io/m1st3rb3an/fairing-job:22FAAB03\n",
      "Pushed image index.docker.io/m1st3rb3an/fairing-job:22FAAB03 in 4.188816641999438s.\n",
      "Training job fairing-job-ftz62 launched.\n",
      "Waiting for fairing-job-ftz62-tfxqh to start...\n",
      "Waiting for fairing-job-ftz62-tfxqh to start...\n",
      "Waiting for fairing-job-ftz62-tfxqh to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0603 20:26:32.434225 139934835124032 <ipython-input-15-d09545b67cbb>:33] Start training\n",
      "W0603 20:26:32.434432 139934835124032 deprecation.py:323] From <ipython-input-15-d09545b67cbb>:34: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0603 20:26:32.434516 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0603 20:26:32.434799 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0603 20:26:32.652960 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0603 20:26:32.971831 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0603 20:26:33.152390 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0603 20:26:33.397658 139934835124032 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:59: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0603 20:26:33.430518 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:96: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0603 20:26:33.431540 139934835124032 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:97: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "W0603 20:26:33.441819 139934835124032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0603 20:26:33.450728 139934835124032 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:118: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0603 20:26:33.452326 139934835124032 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:120: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "2019-06-03 20:26:33.529888: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2019-06-03 20:26:33.538744: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-06-03 20:26:33.539353: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a93f80 executing computations on platform Host. Devices:\n",
      "2019-06-03 20:26:33.539389: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-03 20:26:33.576584: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I0603 20:26:33.579431 139934835124032 <ipython-input-15-d09545b67cbb>:49] Start training\n",
      "I0603 20:26:37.791149 139934835124032 <ipython-input-15-d09545b67cbb>:64] Save my model\n",
      "I0603 20:26:37.834705 139934835124032 <ipython-input-15-d09545b67cbb>:67] End training\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "At step 0, loss = 2.3238918781280518\n",
      "At step 100, loss = 0.45868369936943054\n",
      "At step 200, loss = 0.31672099232673645\n",
      "At step 300, loss = 0.2988934814929962\n",
      "At step 400, loss = 0.17031902074813843\n",
      "At step 500, loss = 0.13957415521144867\n",
      "At step 600, loss = 0.18432337045669556\n",
      "At step 700, loss = 0.06654584407806396\n",
      "At step 800, loss = 0.09690597653388977\n",
      "At step 900, loss = 0.026687823235988617\n",
      "At step 1000, loss = 0.12957197427749634\n",
      "At step 1100, loss = 0.09047866612672806\n",
      "At step 1200, loss = 0.038523197174072266\n",
      "At step 1300, loss = 0.057757288217544556\n",
      "At step 1400, loss = 0.15521559119224548\n",
      "At step 1500, loss = 0.049528807401657104\n",
      "At step 1600, loss = 0.08759113401174545\n",
      "At step 1700, loss = 0.03517787903547287\n",
      "At step 1800, loss = 0.0200914666056633\n",
      "At step 1900, loss = 0.044043492525815964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning up job fairing-job-ftz62...\n"
     ]
    }
   ],
   "source": [
    "remote_train = fairing.config.fn(train)\n",
    "remote_train()\n",
    "#fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <class 'fairing.preprocessors.function.FunctionPreProcessor'>\n",
      "Using docker registry: index.docker.io/m1st3rb3an\n",
      "Using builder: <class 'fairing.builders.append.append.AppendBuilder'>\n",
      "Building the docker image.\n",
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_6y7sy8vt\n",
      "/home/jovyan/fairing/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'tensorflow/tensorflow:nightly-py3-jupyter'\n",
      "Image successfully built in 1.3038988529997368s.\n",
      "Pushing image index.docker.io/m1st3rb3an/fairing-job:2E2F0406...\n",
      "Loading Docker credentials for repository 'index.docker.io/m1st3rb3an/fairing-job:2E2F0406'\n",
      "Uploading index.docker.io/m1st3rb3an/fairing-job:2E2F0406\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:868fe117e0a60fd3c6c0561ee074e68b8f53339cfb2f56ac511f6880a6c9287f exists, skipping\n",
      "Layer sha256:526bb5d0ae6c26ed478aa60ae185d5652e5c8897638368943d992b78a52b1531 exists, skipping\n",
      "Layer sha256:32306fc019abe59ddb505e0b648d83c806bc080f48169987cd0d72e3a8c7db17 exists, skipping\n",
      "Layer sha256:e0b421c621114fc389e8dfa0414c80c2f793e7dc2efe2d449830890bb21afd83 exists, skipping\n",
      "Layer sha256:4bbf71d7b8bf0ea8c97466e83817b8119fdd79ac8249a160d5e89ded63ad2305 exists, skipping\n",
      "Layer sha256:30c30f323f555817df41239125e472e408a81167b64a7039de0054ec28e33570 exists, skipping\n",
      "Layer sha256:b6ee0bfb9b1c368168d47e23c9d657f47b9040cdcf1f49a8819c2adfa50a860d exists, skipping\n",
      "Layer sha256:db7d1dfba811e25d72d24f10c0c98b43775d172401ea7ddea0db5753b0886455 exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:6935b877738aa3f50dc8111baf153e1b4a9425588b26e105e8cb4f350b4999ba exists, skipping\n",
      "Layer sha256:e15daa901f894a3f4e4ace41b4b5b7c7458a5b9293b0f3c91c5fcf107badbe89 exists, skipping\n",
      "Layer sha256:017c56521e603ecbd6287c799a3fc682c034b3f30c2b4531baef4b41c21f3f34 exists, skipping\n",
      "Layer sha256:4922b6b5f9430be9267ee7dcdc4277a95c6e108d815bf62e35d0e4dc3e5eccdf exists, skipping\n",
      "Layer sha256:dadf115bb905dfb34282c8ba43fbc833f1320048bb5c21fb4977b535c862238e exists, skipping\n",
      "Layer sha256:ec20aa1f601bda33bf3452f5d72e98f118ffbbdd978de9e9be51f1cc4333ca65 exists, skipping\n",
      "Layer sha256:435543f18186ca45bb39d5657d6d36ab6ea54372a738996d91024e5d890dc663 exists, skipping\n",
      "Layer sha256:d3f438728c991a3a8d06c7f640f7d142521ef3beda9312dabc92ad09803b2833 exists, skipping\n",
      "Layer sha256:563f85a06a6afea926f1110be2367fa5664bd77fcb7480ac936f514e497962ef exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:ef2fadf9b3940ee85ce37bf07a64871056bc09f82538ae7b195f0a946bdb7e5b exists, skipping\n",
      "Layer sha256:aa2c4e926b79ea0b979763e5362395531fde1e4a6c373b1e89ff34b9f545b51b pushed.\n",
      "Finished upload of: index.docker.io/m1st3rb3an/fairing-job:2E2F0406\n",
      "Pushed image index.docker.io/m1st3rb3an/fairing-job:2E2F0406 in 3.8042705150000984s.\n",
      "Training job fairing-job-n5lsx launched.\n",
      "Waiting for fairing-job-n5lsx-9pqvk to start...\n",
      "Waiting for fairing-job-n5lsx-9pqvk to start...\n",
      "Waiting for fairing-job-n5lsx-9pqvk to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0603 20:26:52.583691 139972787337024 <ipython-input-15-d09545b67cbb>:33] Start training\n",
      "W0603 20:26:52.583896 139972787337024 deprecation.py:323] From <ipython-input-15-d09545b67cbb>:34: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0603 20:26:52.583984 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0603 20:26:52.584267 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0603 20:26:52.796294 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0603 20:26:53.110403 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0603 20:26:53.299218 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0603 20:26:53.543419 139972787337024 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:59: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0603 20:26:53.578820 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:96: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0603 20:26:53.579794 139972787337024 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:97: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "W0603 20:26:53.590769 139972787337024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0603 20:26:53.599965 139972787337024 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:118: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0603 20:26:53.601799 139972787337024 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:120: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "2019-06-03 20:26:53.682224: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2019-06-03 20:26:53.689732: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-06-03 20:26:53.690309: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1a7f280 executing computations on platform Host. Devices:\n",
      "2019-06-03 20:26:53.690344: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-03 20:26:53.726912: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I0603 20:26:53.730260 139972787337024 <ipython-input-15-d09545b67cbb>:49] Start training\n",
      "I0603 20:26:57.913815 139972787337024 <ipython-input-15-d09545b67cbb>:64] Save my model\n",
      "I0603 20:26:57.957621 139972787337024 <ipython-input-15-d09545b67cbb>:67] End training\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "At step 0, loss = 2.316016912460327\n",
      "At step 100, loss = 0.22434145212173462\n",
      "At step 200, loss = 0.18443936109542847\n",
      "At step 300, loss = 0.29757004976272583\n",
      "At step 400, loss = 0.1324886977672577\n",
      "At step 500, loss = 0.21581459045410156\n",
      "At step 600, loss = 0.27860206365585327\n",
      "At step 700, loss = 0.17545916140079498\n",
      "At step 800, loss = 0.1278892457485199\n",
      "At step 900, loss = 0.08880625665187836\n",
      "At step 1000, loss = 0.14712101221084595\n",
      "At step 1100, loss = 0.16188615560531616\n",
      "At step 1200, loss = 0.08006007224321365\n",
      "At step 1300, loss = 0.08640174567699432\n",
      "At step 1400, loss = 0.04151628166437149\n",
      "At step 1500, loss = 0.039139118045568466\n",
      "At step 1600, loss = 0.1618429720401764\n",
      "At step 1700, loss = 0.12397731840610504\n",
      "At step 1800, loss = 0.07401402294635773\n",
      "At step 1900, loss = 0.08423180878162384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning up job fairing-job-n5lsx...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairing-job-n5lsx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fairing import TrainJob\n",
    "from fairing.backends import KubeflowBackend, KubernetesBackend\n",
    "train_job = TrainJob(train, BASE_IMAGE,# input_files=['ames_dataset/train.csv', \"requirements.txt\"],\n",
    "                     docker_registry=DOCKER_REGISTRY, backend=KubernetesBackend())\n",
    "train_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
