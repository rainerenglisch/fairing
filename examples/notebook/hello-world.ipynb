{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/fairing', '/opt/conda/lib/python36.zip', '/opt/conda/lib/python3.6', '/opt/conda/lib/python3.6/lib-dynload', '', '/opt/conda/lib/python3.6/site-packages', '/opt/conda/lib/python3.6/site-packages/IPython/extensions', '/home/jovyan/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/home/jovyan/fairing\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.debug('This will get logged')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job'.format(GCP_PROJECT)\n",
    "fairing.config.set_builder('append',base_image='tensorflow/tensorflow:latest-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "fairing.config.set_deployer('job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "#GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "#DOCKER_REGISTRY = 'mtr.external.otc.telekomcloud.com/rainer_englisch'\n",
    "#DOCKER_REGISTRY = 'docker.io/m1st3rb3an'\n",
    "#DOCKER_REGISTRY = 'registry.hub.docker.com/m1st3rb3an'\n",
    "DOCKER_REGISTRY = 'index.docker.io/m1st3rb3an'\n",
    "\n",
    "BASE_IMAGE ='tensorflow/tensorflow:nightly-py3-jupyter'\n",
    "#fairing.config.set_builder('append',base_image='tensorflow/tensorflow:latest-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "# Rainer: need to use latest base_image because of newer python. The older version in base image is not compatible with one in notebook in regards to serialization\n",
    "#fairing.config.set_builder('append',base_image='tensorflow/tensorflow:devel-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "#fairing.config.set_builder('append',base_image='tensorflow/tensorflow:1.13.1-py3', registry=DOCKER_REGISTRY, push=True)\n",
    "fairing.config.set_builder('append',base_image=BASE_IMAGE, registry=DOCKER_REGISTRY, push=True)\n",
    "fairing.config.set_deployer('job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains and Evaluates the MNIST network using a feed dictionary.\"\"\"\n",
    "import os\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist\n",
    "\n",
    "\n",
    "#import fairing\n",
    "#fairing.config.set_builder('docker', registry='<your-registry-name>',    base_image='tensorflow/tensorflow:1.13.1-py3')\n",
    "#fairing.config.run()\n",
    "\n",
    "INPUT_DATA_DIR = '/tmp/tensorflow/mnist/input_data/'\n",
    "MAX_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.3\n",
    "HIDDEN_1 = 128\n",
    "HIDDEN_2 = 32\n",
    "\n",
    "# HACK: Ideally we would want to have a unique subpath for each instance of the job, but since we can't\n",
    "# we are instead appending HOSTNAME to the logdir\n",
    "LOG_DIR = os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                       'tensorflow/mnist/logs/fully_connected_feed/', os.getenv('HOSTNAME', ''))\n",
    "MODEL_DIR = os.path.join(LOG_DIR, 'model.ckpt')\n",
    "\n",
    "def train():\n",
    "    \n",
    "    #logger.info(\"Importing libs\")\n",
    "    #import tensorflow as tf\n",
    "    #from tensorflow.examples.tutorials.mnist import input_data\n",
    "    #from tensorflow.examples.tutorials.mnist import mnist\n",
    "    logger.info(\"Start training\")\n",
    "    data_sets = input_data.read_data_sets(INPUT_DATA_DIR)\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(BATCH_SIZE, mnist.IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(BATCH_SIZE))\n",
    "\n",
    "    logits = mnist.inference(images_placeholder, HIDDEN_1, HIDDEN_2)\n",
    "    \n",
    "    loss = mnist.loss(logits, labels_placeholder)\n",
    "    train_op = mnist.training(loss, LEARNING_RATE)\n",
    "    summary = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n",
    "    sess.run(init)\n",
    "    \n",
    "    data_set = data_sets.train\n",
    "    logger.info(\"Start training\")\n",
    "    for step in xrange(MAX_STEPS):\n",
    "        images_feed, labels_feed = data_set.next_batch(BATCH_SIZE, False)\n",
    "        feed_dict = {\n",
    "            images_placeholder: images_feed,\n",
    "            labels_placeholder: labels_feed,\n",
    "        }\n",
    "\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                                 feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"At step {}, loss = {}\".format(step, loss_value))\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "    logger.info(\"Save my model\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, 'my_mnist_model')\n",
    "    logger.info(\"End training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local training\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <fairing.preprocessors.function.FunctionPreProcessor object at 0x7f157bfb7320>\n",
      "Using builder: <fairing.builders.append.append.AppendBuilder object at 0x7f157bfb7630>\n",
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_48j6v3_w\n",
      "/home/jovyan/fairing/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'tensorflow/tensorflow:nightly-py3-jupyter'\n",
      "Image successfully built in 1.2929865070000233s.\n",
      "Pushing image index.docker.io/m1st3rb3an/fairing-job:1BA7DC3F...\n",
      "Loading Docker credentials for repository 'index.docker.io/m1st3rb3an/fairing-job:1BA7DC3F'\n",
      "Uploading index.docker.io/m1st3rb3an/fairing-job:1BA7DC3F\n",
      "Layer sha256:7d0e48de88190d6dd210675bcd441cd650354f97044845a4218976590ae4d6e5 exists, skipping\n",
      "Layer sha256:51c539dc4545309d7f8176de523456d26c13f248e41a3692f6902069717d1367 exists, skipping\n",
      "Layer sha256:57f52d349fa954ac1379f8c735f39159fb72a558dbc46c4d0c29c31a18e99d55 exists, skipping\n",
      "Layer sha256:80ca3b488f2477b1dc322135a80d9e2f383c45334a1a4089c8487bff3e06b940 exists, skipping\n",
      "Layer sha256:2434c439fa2a16c7bba143269bc11f7065b3344eb644b37aa0c408c769b2c4cc exists, skipping\n",
      "Layer sha256:330c61d36e2411c0ebb8a46bffd4478092eb31f17cbc92b23bf2239d14a556fd exists, skipping\n",
      "Layer sha256:6ac5ce78f77966c489898c984acb40e2c15af536288e3a1c18ff700d90ac2c23 exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:9b93f0fec4e7fa0953bcbcb0a22a8db4a6b2a46157f896d06fda306f41b91dd6 exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:c4c1c0ba4a62446759f145808528053bf77839a7c008f820d392cd7fef7b8df4 exists, skipping\n",
      "Layer sha256:27281316d11f09b4b3a7873dc00c51e9590dbefc506d2865a00fce292adc4c74 exists, skipping\n",
      "Layer sha256:95a79a395d69d41b8dcd467aed0e9ec856000166912c45749d6dafd7604f16bc exists, skipping\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:821d32a957d81075c0f5e2d68e3a23a96e66c18cb71f4b69c0067eece743c1b2 exists, skipping\n",
      "Layer sha256:89dadf6a51498731dbce86eafb0524ac81c32aee7fc7be57a36565ee5a2188e0 exists, skipping\n",
      "Layer sha256:ae8623a8d8826a41cf2a31ac7ac41bffaec8409ffdc8e2f5f23ed05ce14ba12e exists, skipping\n",
      "Layer sha256:3c8050b9fa8f23441c343d98977075e9bd4608c47c714ff899e87facd69b741e exists, skipping\n",
      "Layer sha256:fe7b7ffb7828af5026340667a3ca658c928d5c8fed9f22983b5b6ed603c0cfae exists, skipping\n",
      "Layer sha256:666d38de74106683ecd7e9aed0ad4c0642697a1b2072c0c57b309a6cf0c6fa32 pushed.\n",
      "Layer sha256:f200dfbe63cf69ba9ccfd416d566c62be34cc25da12030d8e6a6baf2f96e0cfe exists, skipping\n",
      "Layer sha256:33dfa4ed0b7b9964533e2024a95754dc0e22f49cff173bd0a7fa615cd22a49df pushed.\n",
      "Finished upload of: index.docker.io/m1st3rb3an/fairing-job:1BA7DC3F\n",
      "Pushed image index.docker.io/m1st3rb3an/fairing-job:1BA7DC3F in 3.063014275000114s.\n",
      "Training job fairing-job-gqfmf launched.\n",
      "Waiting for fairing-job-gqfmf-lqzj5 to start...\n",
      "Waiting for fairing-job-gqfmf-lqzj5 to start...\n",
      "Waiting for fairing-job-gqfmf-lqzj5 to start...\n",
      "Waiting for fairing-job-gqfmf-lqzj5 to start...\n",
      "Pod started running True\n",
      "Cleaning up job fairing-job-gqfmf...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0604 14:58:02.050862 140564877014848 <ipython-input-6-d09545b67cbb>:33] Start training\n",
      "W0604 14:58:02.051071 140564877014848 deprecation.py:323] From <ipython-input-6-d09545b67cbb>:34: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0604 14:58:02.051145 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0604 14:58:02.051444 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0604 14:58:04.146860 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0604 14:58:04.464809 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0604 14:58:04.642082 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0604 14:58:04.884614 140564877014848 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:59: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0604 14:58:04.917700 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:96: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0604 14:58:04.918636 140564877014848 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:97: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "W0604 14:58:04.928842 140564877014848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0604 14:58:04.937785 140564877014848 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:118: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0604 14:58:04.939212 140564877014848 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:120: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "2019-06-04 14:58:05.016738: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2019-06-04 14:58:05.026827: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-06-04 14:58:05.027446: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x39028d0 executing computations on platform Host. Devices:\n",
      "2019-06-04 14:58:05.027480: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-04 14:58:05.064418: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I0604 14:58:05.068856 140564877014848 <ipython-input-6-d09545b67cbb>:49] Start training\n",
      "I0604 14:58:09.292943 140564877014848 <ipython-input-6-d09545b67cbb>:64] Save my model\n",
      "I0604 14:58:09.335673 140564877014848 <ipython-input-6-d09545b67cbb>:67] End training\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "At step 0, loss = 2.283696174621582\n",
      "At step 100, loss = 0.6571828722953796\n",
      "At step 200, loss = 0.47795408964157104\n",
      "At step 300, loss = 0.33201637864112854\n",
      "At step 400, loss = 0.2365141063928604\n",
      "At step 500, loss = 0.2427920550107956\n",
      "At step 600, loss = 0.14939075708389282\n",
      "At step 700, loss = 0.09107589721679688\n",
      "At step 800, loss = 0.0963236540555954\n",
      "At step 900, loss = 0.13915136456489563\n",
      "At step 1000, loss = 0.05679458752274513\n",
      "At step 1100, loss = 0.04190569370985031\n",
      "At step 1200, loss = 0.12793011963367462\n",
      "At step 1300, loss = 0.04963488504290581\n",
      "At step 1400, loss = 0.07897496968507767\n",
      "At step 1500, loss = 0.046707019209861755\n",
      "At step 1600, loss = 0.18638256192207336\n",
      "At step 1700, loss = 0.08057195693254471\n",
      "At step 1800, loss = 0.12252947688102722\n",
      "At step 1900, loss = 0.12233662605285645\n"
     ]
    }
   ],
   "source": [
    "remote_train = fairing.config.fn(train)\n",
    "remote_train()\n",
    "#fairing.config.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <class 'fairing.preprocessors.function.FunctionPreProcessor'>\n",
      "Using docker registry: index.docker.io/m1st3rb3an\n",
      "Using builder: <class 'fairing.builders.append.append.AppendBuilder'>\n",
      "Building the docker image.\n",
      "Building image using Append builder...\n",
      "Creating docker context: /tmp/fairing_context_boxjz84v\n",
      "/home/jovyan/fairing/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Loading Docker credentials for repository 'tensorflow/tensorflow:nightly-py3-jupyter'\n",
      "Image successfully built in 1.2580996300000606s.\n",
      "Pushing image index.docker.io/m1st3rb3an/fairing-job:7915AEE8...\n",
      "Loading Docker credentials for repository 'index.docker.io/m1st3rb3an/fairing-job:7915AEE8'\n",
      "Uploading index.docker.io/m1st3rb3an/fairing-job:7915AEE8\n",
      "Layer sha256:7d0e48de88190d6dd210675bcd441cd650354f97044845a4218976590ae4d6e5 exists, skipping\n",
      "Layer sha256:33dfa4ed0b7b9964533e2024a95754dc0e22f49cff173bd0a7fa615cd22a49df exists, skipping\n",
      "Layer sha256:6ac5ce78f77966c489898c984acb40e2c15af536288e3a1c18ff700d90ac2c23 exists, skipping\n",
      "Layer sha256:51c539dc4545309d7f8176de523456d26c13f248e41a3692f6902069717d1367 exists, skipping\n",
      "Layer sha256:57f52d349fa954ac1379f8c735f39159fb72a558dbc46c4d0c29c31a18e99d55 exists, skipping\n",
      "Layer sha256:80ca3b488f2477b1dc322135a80d9e2f383c45334a1a4089c8487bff3e06b940 exists, skipping\n",
      "Layer sha256:2434c439fa2a16c7bba143269bc11f7065b3344eb644b37aa0c408c769b2c4cc exists, skipping\n",
      "Layer sha256:330c61d36e2411c0ebb8a46bffd4478092eb31f17cbc92b23bf2239d14a556fd exists, skipping\n",
      "Layer sha256:9b93f0fec4e7fa0953bcbcb0a22a8db4a6b2a46157f896d06fda306f41b91dd6 exists, skipping\n",
      "Layer sha256:6abc03819f3e00a67ed5adc1132cfec041d5f7ec3c29d5416ba0433877547b6f exists, skipping\n",
      "Layer sha256:05731e63f21105725a5c062a725b33a54ad8c697f9c810870c6aa3e3cd9fb6a2 exists, skipping\n",
      "Layer sha256:0bd67c50d6beeb55108476f72bea3b4b29a9f48832d6e045ec66b7ac4bf712a0 exists, skipping\n",
      "Layer sha256:27281316d11f09b4b3a7873dc00c51e9590dbefc506d2865a00fce292adc4c74 exists, skipping\n",
      "Layer sha256:c4c1c0ba4a62446759f145808528053bf77839a7c008f820d392cd7fef7b8df4 exists, skipping\n",
      "Layer sha256:821d32a957d81075c0f5e2d68e3a23a96e66c18cb71f4b69c0067eece743c1b2 exists, skipping\n",
      "Layer sha256:95a79a395d69d41b8dcd467aed0e9ec856000166912c45749d6dafd7604f16bc exists, skipping\n",
      "Layer sha256:3c8050b9fa8f23441c343d98977075e9bd4608c47c714ff899e87facd69b741e exists, skipping\n",
      "Layer sha256:89dadf6a51498731dbce86eafb0524ac81c32aee7fc7be57a36565ee5a2188e0 exists, skipping\n",
      "Layer sha256:ae8623a8d8826a41cf2a31ac7ac41bffaec8409ffdc8e2f5f23ed05ce14ba12e exists, skipping\n",
      "Layer sha256:f200dfbe63cf69ba9ccfd416d566c62be34cc25da12030d8e6a6baf2f96e0cfe exists, skipping\n",
      "Layer sha256:fe7b7ffb7828af5026340667a3ca658c928d5c8fed9f22983b5b6ed603c0cfae exists, skipping\n",
      "Layer sha256:929821cc8a55724e7eb0b26ed6153c0f82e5109ecbcdd541b617b61445b15ad0 pushed.\n",
      "Finished upload of: index.docker.io/m1st3rb3an/fairing-job:7915AEE8\n",
      "Pushed image index.docker.io/m1st3rb3an/fairing-job:7915AEE8 in 3.3008913190001294s.\n",
      "Training job fairing-job-rx2fb launched.\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Waiting for fairing-job-rx2fb-w9nkc to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0604 15:03:23.264389 140343499114304 <ipython-input-6-d09545b67cbb>:33] Start training\n",
      "W0604 15:03:23.264589 140343499114304 deprecation.py:323] From <ipython-input-6-d09545b67cbb>:34: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0604 15:03:23.264686 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0604 15:03:23.264944 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0604 15:03:23.500130 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0604 15:03:23.820089 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0604 15:03:24.001139 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0604 15:03:24.248340 140343499114304 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:59: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0604 15:03:24.283285 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:96: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0604 15:03:24.284220 140343499114304 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:97: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "W0604 15:03:24.295274 140343499114304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0604 15:03:24.304730 140343499114304 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:118: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0604 15:03:24.306225 140343499114304 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow/examples/tutorials/mnist/mnist.py:120: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "2019-06-04 15:03:24.387242: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2019-06-04 15:03:24.394803: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-06-04 15:03:24.395216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44ae050 executing computations on platform Host. Devices:\n",
      "2019-06-04 15:03:24.395250: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-04 15:03:24.432024: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I0604 15:03:24.435178 140343499114304 <ipython-input-6-d09545b67cbb>:49] Start training\n",
      "I0604 15:03:28.569061 140343499114304 <ipython-input-6-d09545b67cbb>:64] Save my model\n",
      "I0604 15:03:28.612914 140343499114304 <ipython-input-6-d09545b67cbb>:67] End training\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "At step 0, loss = 2.3021979331970215\n",
      "At step 100, loss = 0.4447309076786041\n",
      "At step 200, loss = 0.17961224913597107\n",
      "At step 300, loss = 0.2010989636182785\n",
      "At step 400, loss = 0.08293995261192322\n",
      "At step 500, loss = 0.18774916231632233\n",
      "At step 600, loss = 0.12451859563589096\n",
      "At step 700, loss = 0.14473137259483337\n",
      "At step 800, loss = 0.10112743079662323\n",
      "At step 900, loss = 0.06156335398554802\n",
      "At step 1000, loss = 0.05541209131479263\n",
      "At step 1100, loss = 0.100460484623909\n",
      "At step 1200, loss = 0.12649749219417572\n",
      "At step 1300, loss = 0.1619747281074524\n",
      "At step 1400, loss = 0.0378582589328289\n",
      "At step 1500, loss = 0.1441245824098587\n",
      "At step 1600, loss = 0.10811953246593475\n",
      "At step 1700, loss = 0.07477743178606033\n",
      "At step 1800, loss = 0.016236502677202225\n",
      "At step 1900, loss = 0.09403856098651886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning up job fairing-job-rx2fb...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fairing-job-rx2fb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fairing import TrainJob\n",
    "from fairing.backends import KubeflowBackend, KubernetesBackend\n",
    "train_job = TrainJob(train, BASE_IMAGE,# input_files=['ames_dataset/train.csv', \"requirements.txt\"],\n",
    "                     docker_registry=DOCKER_REGISTRY, backend=KubernetesBackend())\n",
    "train_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
